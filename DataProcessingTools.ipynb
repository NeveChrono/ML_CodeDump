{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE2ANaVOZTEOFdItwGx6dH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeveChrono/ML_CodeDump/blob/main/DataProcessingTools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing Tools\n"
      ],
      "metadata": {
        "id": "h_PUEUFeMxj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries\n",
        "*   Numpy - work with arrays\n",
        "* Pandas - Import the dataset and create a matrix set\n",
        "* Mathplotlib - Use for visualization\n",
        "*   Scikit learn - Most popular library which is popularly used in ML\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jkOeYQv_M91M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5JiN9emSNgRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import the Dataset"
      ],
      "metadata": {
        "id": "cbHlzAWeN8TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a variable which has a dataset ie in the form of a dataframe\n",
        "\n",
        "dataset = pd.read_csv('Data.csv')\n",
        "\n",
        "''' In any training model we have two things to work with one is features and\n",
        "the dependent variables which will give me the output. In this case of Data.csv is the\n",
        "date of weather a car was purchased by a person of a certain age country and salary\n",
        "So,\n",
        " Features = Country Age and Salary\n",
        " Dependent Variable = Purchased\n",
        "\n",
        " So we will make prediction if they will purchased or not. Generally features are are written in the first columns\n",
        " and dependent variables are written in the last column.\n",
        "'''\n",
        "# entities:\n",
        "\n",
        "# locate the data in the set based on the index , it has two things [row range , column range]\n",
        "# Basically will select all the rows and columns upto the last one as the last column is not included\n",
        "X = dataset.iloc[:,:-1].values\n",
        "Y = dataset.iloc[:,-1].values\n",
        "\n",
        "# we need to use .values to extract it out\n"
      ],
      "metadata": {
        "id": "Pz6C3F6iOLb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To check the values we extracted."
      ],
      "metadata": {
        "id": "guBMNfgsRpih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Independent Variable or Features\")\n",
        "print(X)\n",
        "\n",
        "print(\"Dependent Variable\")\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96IkCuMaRW6A",
        "outputId": "80991bd4-d049-44a0-f39a-c3fb11f383cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Independent Variable or Features\n",
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n",
            "Dependent Variable\n",
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To take care of the Missing Data\n"
      ],
      "metadata": {
        "id": "-0NFwMyXRvXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the missing data can cause issues. One way is to ignore that entire reading but it can only work with large dataset having less missing values.\n"
      ],
      "metadata": {
        "id": "V2427NnAT1Xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way is to replace the data with the average of the entire parameter. Using scikit-learn SimpleImputer"
      ],
      "metadata": {
        "id": "SIrYOrjtTpIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "# we will create instance of the class SimpleImputer\n",
        "# this will be taking finding the empty valuesie np.nan will replacing them with the mean values of the column\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "'''\n",
        "to achieve we will use two methods:-\n",
        "1. fit() - finds the missing values and then computes average simultaneously\n",
        "2. transform() - replaces the missing values with the average\n",
        "\n",
        "Since the fit function takes in the data set where it will find the empty number values\n",
        "hence we need to mention which rows to look from out of the entire dataset.\n",
        "That will be col 2 and 3 which is age and salary.\n",
        "\n",
        "As for String and categorical data you need to convert into numerical before u\n",
        "go into the imputer.\n",
        "\n",
        "Common techniques include:\n",
        "\n",
        "    - Label Encoding: Converts each category to a unique integer.\n",
        "    - Ordinal Encoding: Assigns numerical values based on the order of\n",
        "                        categories.\n",
        "    - One-Hot Encoding: Converts categorical values into a binary.\n",
        "\n",
        "    vector with a length equal to the number of unique categories.\n",
        "'''\n",
        "\n",
        "imputer.fit(X[:,1:3])\n",
        "\n",
        "'''\n",
        "Now to transform our dataset by replacing the missing salary\n",
        "ie in the missing value\n",
        "'''\n",
        "X[:,1:3]=imputer.transform(X[:,1:3])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YOkm4I9hUw8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXl-fY8BYDO-",
        "outputId": "c511f7bb-7d97-4a17-f15e-63ff9afd018e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Categorical Data\n"
      ],
      "metadata": {
        "id": "UYd_NruPaeg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Discussed above it pretty clear teh fit function can only take in data which is in numerical so for category values we need to find a way to do it."
      ],
      "metadata": {
        "id": "NAbSkm_ta-ZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Approach would be to well assign each with 0,1,2.. but this can be interpreted as an order which can mess up the output."
      ],
      "metadata": {
        "id": "1VPKn1r4bJ5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the best approach will be to split the column into several sub columns based on each value of the category.\n",
        "\n",
        "Assign it a binary vector like <001> or <010> for each value in the category.\n",
        "\n",
        "This can be applied to n number of categorcial values and is called One-Hot Encoding"
      ],
      "metadata": {
        "id": "S0O9vL-NbTFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will import two classes to achieve it ColumnTransformer and OneHotEncoder\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# create an objects\n",
        "\n",
        "'''\n",
        "The columnTransformer class has two arguments one is transformers and the other is remainder.\n",
        "\n",
        "- transformers basically  tells us what kind of transform we are applying on the column\n",
        "ie encoder to encode it , how the encoding is done and on which index or column name it must be done from that list\n",
        "\n",
        "- remainder ensures that after the transform is done all the classes which where not affected by it are kept\n",
        "using the keyword passthrough\n",
        "\n",
        "   One-Hot Encoding: Converts categorical values into a binary. In case if there are no numeric values take them as false to solve the problem.\n",
        "   and also u can uses indices to mention which index u want but better will be to create a list of columns for which u want to transform\n",
        "\n",
        "'''\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n",
        "\n",
        "'''\n",
        "Unlike for the previous case where we have to use fit and transform seperatly to fill in the data\n",
        "we can use an inbuilt method of ColumnTransfromer class called fit_transform. It does the job but has a catch as it doesnt\n",
        "returns in the value as np.array which what we work with so we need to convert the result into a numpy array\n",
        "'''\n",
        "\n",
        "X = np.array(ct.fit_transform(X))\n",
        "print(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh7DxDPVaoq3",
        "outputId": "6236ecfd-95bd-4063-efbe-99ba29c82161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Output Encoding we will use the label encoder"
      ],
      "metadata": {
        "id": "YolksdGtffKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create an Instance of the class\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# We dont need an numpy array for this so no need for conversion as this is the result\n",
        "Y = le.fit_transform(Y)\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-Gqlxnmfj-9",
        "outputId": "762c971d-11a3-408b-aad4-7024efc672ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split The data into Training and Dataset"
      ],
      "metadata": {
        "id": "lG8hdZ0ygDqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the Dataset using Scikit Learn using a function\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the Data into 4 sets -- Feature Train and Test Set , Dependent Output Train and Test Set\n",
        "\n",
        "'''\n",
        "It basically takes 4 parameters as input two of them being the feature dataset and output\n",
        "dataset as it cannot take in the dataset as a whole , next the test_size which is generally\n",
        "kept as 20% of the dataset as we need more values to train the model. And to ensure proper\n",
        "spliting we set to random_state as one.\n",
        "\n",
        "'''\n",
        "\n",
        "X_train , X_test , Y_train , Y_test = train_test_split(X,Y,test_size = 0.2,random_state=1)"
      ],
      "metadata": {
        "id": "35_-0i2Mmw11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiAC3cgVoheL",
        "outputId": "dd24a54c-c629-476e-fd61-d2684f2c47ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyHeBkTrohno",
        "outputId": "79ab6dce-09aa-476f-b950-c9baaf8b398e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3C53KpNohuI",
        "outputId": "57fe5650-e08a-46f1-de92-abc4e5c37179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrFe77bIoh3S",
        "outputId": "f1b914a4-782b-4304-ef99-90e7d793fb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling\n",
        "\n",
        "When to Apply it before the Spliting or After the Spliting?\n",
        "\n",
        "Basically Speaking Feature Scaling is all about to ensure that the machine learning model plays fair with the given data , where in each of feature used to train it are all on the same scale ensuring that no one feature will domainate the result. Thereby giving us a relative measure of the model and at the same time eliminating any bias.\n",
        "\n",
        "So should this be done before the data split or after the split?\n",
        "\n",
        "We mainly do the spliting so that we have a training set and a test data set to work in. feature Scaling will ensure each of the measure quantities are on same scale so no bias can be found but if do this before the split.\n",
        "\n",
        "There will be some info leak into the Scaling as we all know scaling is done by calculating the deviation or the mean and having data added to it can be problematic as it will also affect in our scaled values and hence a bias\n",
        "\n",
        "There are two types of Feature Scaling:-\n",
        "1. Normalisation\n",
        "2. Standarization\n",
        "\n",
        "Each of them are used for different use cases generally speaking we can say that the first one can be used when the said distrubution follows a normalisation curve.\n",
        "\n",
        "X_scaled = X - X_min / X_max - X_min\n",
        "\n",
        "While the Standarization is more general as it can always work and without any restriction.\n",
        "\n",
        "X_scaled = X - mean(X) / standard_deviation(X)"
      ],
      "metadata": {
        "id": "HRab3ZH9k7-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will again use Scikit learn in which there is a library for feature Scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create an instance of the class\n",
        "\n",
        "sc = StandardScaler() # No parameters to used as it does the job find mean and standard deviation on its own\n",
        "\n",
        "'''\n",
        "Now also to note that the fact when you are applying feature scaling on your dataset it is\n",
        "essential to know that it is better to only apply for the non encoded values seeing that the encoded columns are\n",
        "already existing in the state of 0 or 1 in the dummy variables used to make them. So applying the scaling will not only\n",
        "so it is always better for cleaner data representation and in general training of the model we tend to avoid the dummy variables ie the encoded categorical data.\n",
        "1. Misplace and make it confuse if the given categorical data was in that dummy variable or not\n",
        "2. There wont be much difference if it is not done wrt to if it done.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "X_train[:,3:] = sc.fit_transform(X_train[:,3:])\n",
        "X_test[:,3:] = sc.transform(X_test[:,3:])"
      ],
      "metadata": {
        "id": "6VIceF6czjPI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)\n"
      ],
      "metadata": {
        "id": "ZBuXgbEgzQ1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b519acc-da5c-4252-b036-469b178881b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.1915918438457856 -1.0781259408412427]\n",
            " [0.0 1.0 0.0 -0.014117293757057902 -0.07013167641635401]\n",
            " [1.0 0.0 0.0 0.5667085065333239 0.6335624327104546]\n",
            " [0.0 0.0 1.0 -0.3045301939022488 -0.30786617274297895]\n",
            " [0.0 0.0 1.0 -1.901801144700799 -1.4204636155515822]\n",
            " [1.0 0.0 0.0 1.1475343068237056 1.2326533634535488]\n",
            " [0.0 1.0 0.0 1.4379472069688966 1.5749910381638883]\n",
            " [1.0 0.0 0.0 -0.7401495441200352 -0.5646194287757336]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0jirHxn_rRI",
        "outputId": "e2ce6d03-acd7-4619-9a6a-a380f503d837"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830127 -0.9069571034860731]\n",
            " [1.0 0.0 0.0 -0.44973664397484425 0.20564033932253029]]\n"
          ]
        }
      ]
    }
  ]
}